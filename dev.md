# ğŸ§  NanoChat ë¡œì»¬ í•™ìŠµ ì„¸íŒ… (RTX 3060 í™˜ê²½)

## ğŸ’» ì‘ì„±ì í•˜ë“œì›¨ì–´

| í•­ëª©    | ì‚¬ì–‘                                    |
| ------- | --------------------------------------- |
| **CPU** | Intel Core i7-12700K                    |
| **RAM** | Samsung DDR5 4800MHz 16GB Ã— 2 (ì´ 32GB) |
| **SSD** | Samsung PM9A1 1TB (NVMe)                |
| **VGA** | NVIDIA GeForce RTX 3060 12GB            |
| **OS**  | Windows 11 25H2                         |
| **WSL** | Ubuntu 24.04 LTS                        |

---

## 1ï¸âƒ£ WSL ì§„ì… í›„ ì´ˆê¸° ì„¸íŒ…

```bash
# í”„ë¡œì íŠ¸ í´ë¡  ë° ì§„ì…
cd ~/
git clone https://github.com/karpathy/nanochat.git
cd nanochat

# íŒŒì´ì¬ ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
python3 -m venv venv
source venv/bin/activate

# CUDA ì¸ì‹ í™•ì¸
nvidia-smi
```

> ğŸ’¡ ë§Œì•½ `nvidia-smi` ëª…ë ¹ì´ ì—†ë‹¤ê³  ë‚˜ì˜¤ë©´ ë‹¤ìŒì„ ì‹¤í–‰ í›„ ì¬ë¶€íŒ…:
>
> ```bash
> sudo add-apt-repository ppa:graphics-drivers/ppa
> sudo ubuntu-drivers autoinstall
> sudo reboot
> nvidia-smi
> ```

ì´í›„ ì§„í–‰ì„ ìœ„í•´ í•„ìˆ˜
```bash
cd ~/.cache/nanochat
curl -L -o eval_bundle.zip https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip
unzip eval_bundle.zip
rm eval_bundle.zip
```

---

## 2ï¸âƒ£ ë°ì´í„°ì…‹ ìƒ˜í”Œ ë‹¤ìš´ë¡œë“œ (10 ìƒ¤ë“œë§Œ í…ŒìŠ¤íŠ¸ìš©)

```bash
python -m nanochat.dataset -n 10 -w 2
```

- ê¸°ë³¸ ì €ì¥ ê²½ë¡œ: `~/.cache/nanochat/base_data`
- ì•½ 1GB ë‚´ì™¸ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ë‹¤ìš´ë¡œë“œë¨

---

## 3ï¸âƒ£ Rust ê¸°ë°˜ í† í¬ë‚˜ì´ì € ë¹Œë“œ (`rustbpe`)

```bash
pip install maturin
maturin develop --release --manifest-path rustbpe/Cargo.toml
```

> âœ… ì„±ê³µ ì‹œ ì¶œë ¥ ì˜ˆì‹œ:
>
> ```
> Finished `release` profile [optimized] target(s) in Xs
> ğŸ“¦ Built wheel for CPython 3.12 ...
> ğŸ›  Installed nanochat-0.1.0
> ```

### í† í¬ë‚˜ì´ì € ì •ìƒ ë™ì‘ í™•ì¸

```bash
python - <<'PY'
import rustbpe
print("rustbpe OK. has Tokenizer?", hasattr(rustbpe, "Tokenizer"))
print(dir(rustbpe))
PY
```

> âœ… ì¶œë ¥ ì˜ˆì‹œ
>
> ```
> rustbpe OK. has Tokenizer? True
> ['Tokenizer', '__all__', ...]
> ```

---

## 4ï¸âƒ£ í† í¬ë‚˜ì´ì € í•™ìŠµ ë° í‰ê°€

```bash
python -m scripts.tok_train --max_chars=200000000
python -m scripts.tok_eval
```

> âœ… ì¶œë ¥ ì˜ˆì‹œ:
>
> ```
> Saved tokenizer encoding to ~/.cache/nanochat/tokenizer/tokenizer.pkl
> Ours: vocab_size=65536 (GPT-2: 50257, GPT-4: 100277)
> ```
>
> - ì •ìƒì ìœ¼ë¡œ ì €ì¥ë˜ë©´ í† í¬ë‚˜ì´ì € ë‹¨ê³„ ì™„ë£Œ
> - GPT-2/4 ëŒ€ë¹„ ì••ì¶•ë¥  ë¹„êµ ê²°ê³¼ë„ í‘œì‹œë¨

---

## 5ï¸âƒ£ CUDA ë©”ëª¨ë¦¬ ìµœì í™” í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
export PYTORCH_ALLOC_CONF="expandable_segments:True,max_split_size_mb:64,garbage_collection_threshold:0.8"
```

- PyTorch 2.9 ì´ìƒì—ì„œëŠ” `PYTORCH_CUDA_ALLOC_CONF` ëŒ€ì‹  `PYTORCH_ALLOC_CONF` ì‚¬ìš©
- ë©”ëª¨ë¦¬ ë‹¨í¸í™” ìµœì†Œí™” ë° VRAM ëˆ„ìˆ˜ ë°©ì§€ íš¨ê³¼

---

## 6ï¸âƒ£ Base ëª¨ë¸ ì‚¬ì „í•™ìŠµ (í…ŒìŠ¤íŠ¸ìš© ì†Œí˜• ì„¤ì •)

```bash
# ë°°ì¹˜/ì‹œí€€ìŠ¤ì— ë§ì¶° total_batch_sizeë¥¼ ë°°ìˆ˜ë¡œ ì„¤ì • (384 * 1024 = 393216)
python -m scripts.base_train \
  --run=dummy \
  --depth=6 \
  --max_seq_len=384 \
  --device_batch_size=1 \
  --total_batch_size=393216 \
  --num_iterations=1000
```

### âš™ï¸ ì£¼ìš” íŒŒë¼ë¯¸í„° ì„¤ëª…

| ì˜µì…˜                        | ì„¤ëª…                            |
| --------------------------- | ------------------------------- |
| `--depth=6`                 | Transformer ë¸”ë¡ 6ì¸µ            |
| `--max_seq_len=384`         | ì…ë ¥ ì‹œí€€ìŠ¤ ìµœëŒ€ í† í° ê¸¸ì´      |
| `--device_batch_size=1`     | GPU ë‹¨ì¼ ë°°ì¹˜ (VRAM ì ˆì•½ìš©)     |
| `--total_batch_size=393216` | ë°°ì¹˜/ì‹œí€€ìŠ¤ ê³±ì˜ ë°°ìˆ˜ ì¡°ê±´ í•„ìˆ˜ |
| `--num_iterations=1000`     | í•™ìŠµ ìŠ¤í… ìˆ˜                    |

### ğŸ§© ì‹¤í–‰ ìš”ì•½

- **ì˜ˆìƒ VRAM ì‚¬ìš©ëŸ‰:** 7~9GB
- **ì˜ˆìƒ ì†Œìš”ì‹œê°„:** ì•½ 9\~11ì‹œê°„ (ì„±ëŠ¥ 70% ì œí•œ ì‹œ 12\~14ì‹œê°„)
- **í•™ìŠµ ì¶œë ¥ ê²½ë¡œ:** `./runs/dummy`
- **ë¡œê·¸ ë° ì²´í¬í¬ì¸íŠ¸ ìë™ ì €ì¥**

---

## âš™ï¸ (ì˜µì…˜) VRAM ì œí•œ ì„¤ì •

```bash
# PATH í™˜ê²½ë³€ìˆ˜ì— nvidia-smi ê²½ë¡œ ì¶”ê°€
export PATH="/usr/lib/wsl/lib:$PATH"

# GPU ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi
```

---

## âœ… í•™ìŠµ ì™„ë£Œ í›„

- ê²°ê³¼ ëª¨ë¸ê³¼ ë¡œê·¸ëŠ” `./runs/day1_base_small` ë˜ëŠ” `./runs/dummy` ê²½ë¡œì— ì €ì¥ë¨
- ì´í›„ ë‹¨ê³„:

# ğŸ“š NanoChat ì§„í–‰ í”Œëœ â€” ê¶Œì¥ / íƒ„íƒ„ / ì „ì²´ (ë¶„ê¸° ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ ìš´ì˜)

> âœ… ì „ì œ: ì•„ë˜ ë² ì´ìŠ¤ë¼ì¸(ì´ë¯¸ ì§„í–‰)
>
> ```bash
> # Base Pretrain 1ì°¨: 1,000 ìŠ¤í…(ì™„ë£Œ)
> python -m scripts.base_train \
>   --run=dummy \
>   --depth=6 \
>   --max_seq_len=384 \
>   --device_batch_size=1 \
>   --total_batch_size=393216 \
>   --num_iterations=1000
> ```

---

## ğŸ”€ ë¶„ê¸°(Branch) ì €ì¥/ì¬ê°œ â€” ê³µí†µ ë£¨í‹´

> ê° Day ì¢…ë£Œ ì§í›„ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ë¥¼ ë³€ìˆ˜ì— ë‹´ì•„ **ë¶„ê¸°ìš©ìœ¼ë¡œ ë³´ê´€**í•©ë‹ˆë‹¤.  
> ì¼ë¶€ ìŠ¤í¬ë¦½íŠ¸ëŠ” `--init_from`ë¥¼ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. **ì§€ì›ë˜ë©´ ì‚¬ìš©**, ì—†ìœ¼ë©´ **ë™ì¼ `--run`ìœ¼ë¡œ ì´ì–´ì„œ** ì§„í–‰í•˜ì„¸ìš”.

```bash
# ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì¡ê¸°
CKPT=$(ls -1t runs/dummy/*.pt 2>/dev/null | head -n 1)
echo "LATEST CKPT: $CKPT"

# (ì˜µì…˜) ì•ˆì „ ë³´ê´€ìš© ë³µì‚¬
mkdir -p ckpt_archive
[ -n "$CKPT" ] && cp -v "$CKPT" ckpt_archive/

# (ì˜µì…˜) ë¶„ê¸°ìš© run ì´ë¦„ìœ¼ë¡œ ì‹œì‘í•˜ê³  ì‹¶ìœ¼ë©´
# ë™ì¼ ì˜µì…˜ + --run=<ìƒˆì´ë¦„> ìœ¼ë¡œ ì¬ì‹œì‘ (ì§€ì› ì‹œ --init_from ì‚¬ìš©)
# ì˜ˆì‹œ:
# python -m scripts.base_train \
#   --run=dummy_branchA \
#   --max_seq_len=384 \
#   --device_batch_size=1 \
#   --total_batch_size=393216 \
#   --num_iterations=500 \
#   --init_from="$CKPT"     # ìŠ¤í¬ë¦½íŠ¸ì— ì—†ìœ¼ë©´ ì´ ì¤„ì€ ë¹¼ì„¸ìš”
```

---

# ğŸŸ¢ ê¶Œì¥(Recommended) ë²„ì „ â€” ì•½ 1~2ì¼ / 6h ë¸”ë¡ Ã— 2~3íšŒ

### Day 1 (6h)

**Base Pretrain 1ì°¨: +1,500 ìŠ¤í… (ëˆ„ì  ~2,500 / ~0.98B í† í°)**

```bash
export PYTORCH_ALLOC_CONF="expandable_segments:True,max_split_size_mb:64,garbage_collection_threshold:0.8"

python -m scripts.base_train \
  --run=dummy \
  --depth=6 \
  --max_seq_len=384 \
  --device_batch_size=1 \
  --total_batch_size=393216 \
  --num_iterations=1500
```

**ë¶„ê¸° ì €ì¥**

```bash
CKPT=$(ls -1t runs/dummy/*.pt 2>/dev/null | head -n 1)
echo "LATEST CKPT: $CKPT"
mkdir -p ckpt_archive && [ -n "$CKPT" ] && cp -v "$CKPT" ckpt_archive/
```

---

### Day 2 (6h)

**Base Pretrain 2ì°¨: +500 ìŠ¤í… (ì„ íƒ, ì—¬ìœ ì‹œê°„ìš©)**

> ëˆ„ì  3,000 ìŠ¤í…ê¹Œì§€ ì˜¬ë¦¬ë©´ ì¼ë°˜ì ìœ¼ë¡œ ì•ˆì •ê° â†‘ (ì›í•˜ë©´ ìƒëµ ê°€ëŠ¥)

```bash
python -m scripts.base_train \
  --run=dummy \
  --max_seq_len=384 \
  --device_batch_size=1 \
  --total_batch_size=393216 \
  --num_iterations=500
```

**ë¹ ë¥¸ í‰ê°€**

```bash
python -m scripts.base_eval   # ë ˆí¬ ë²„ì „ì— ë”°ë¼ ìœ íš¨/ë¬´íš¨
python -m scripts.base_loss   # ë¡œìŠ¤/ìƒ˜í”Œ í™•ì¸
```

**ë¶„ê¸° ì €ì¥**

```bash
CKPT=$(ls -1t runs/dummy/*.pt 2>/dev/null | head -n 1)
echo "LATEST CKPT: $CKPT"
mkdir -p ckpt_archive && [ -n "$CKPT" ] && cp -v "$CKPT" ckpt_archive/
```

---

### Day 3 (6h)

**Mid-Train 1ì°¨: 1,000 ìŠ¤í…**

```bash
python -m scripts.mid_train \
  --run=dummy \
  --device_batch_size=1 \
  --max_steps=1000
python -m scripts.chat_eval -- -i mid
```

**SFT 1ì°¨: 1,000 ìŠ¤í…**

```bash
python -m scripts.chat_sft \
  --run=dummy \
  --device_batch_size=1 \
  --max_steps=1000
python -m scripts.chat_eval -- -i sft
```

**ì›¹UI**

```bash
python -m scripts.chat_web
# http://127.0.0.1:8000
```

---

# ğŸ”µ íƒ„íƒ„(Solid) ë²„ì „ â€” ì•½ 2~3ì¼ / 6h ë¸”ë¡ Ã— 3~4íšŒ

### Day 1 (6h)

**Base Pretrain 1ì°¨: +1,500 ìŠ¤í… (ëˆ„ì  ~2,500)**

```bash
export PYTORCH_ALLOC_CONF="expandable_segments:True,max_split_size_mb:64,garbage_collection_threshold:0.8"

python -m scripts.base_train \
  --run=dummy \
  --depth=6 \
  --max_seq_len=384 \
  --device_batch_size=1 \
  --total_batch_size=393216 \
  --num_iterations=1500
```

**ë¶„ê¸° ì €ì¥**

```bash
CKPT=$(ls -1t runs/dummy/*.pt 2>/dev/null | head -n 1)
echo "LATEST CKPT: $CKPT"
mkdir -p ckpt_archive && [ -n "$CKPT" ] && cp -v "$CKPT" ckpt_archive/
```

---

### Day 2 (6h)

**Base Pretrain 2ì°¨: +1,500 ìŠ¤í… (ëˆ„ì  ~4,000 / ~1.57B í† í°)**

```bash
python -m scripts.base_train \
  --run=dummy \
  --max_seq_len=384 \
  --device_batch_size=1 \
  --total_batch_size=393216 \
  --num_iterations=1500
```

**ë¹ ë¥¸ í‰ê°€ & ë¶„ê¸° ì €ì¥**

```bash
python -m scripts.base_eval
python -m scripts.base_loss
CKPT=$(ls -1t runs/dummy/*.pt 2>/dev/null | head -n 1)
echo "LATEST CKPT: $CKPT"
mkdir -p ckpt_archive && [ -n "$CKPT" ] && cp -v "$CKPT" ckpt_archive/
```

---

### Day 3 (6h)

**Mid-Train 1ì°¨: 1,000 ìŠ¤í…**

```bash
python -m scripts.mid_train \
  --run=dummy \
  --device_batch_size=1 \
  --max_steps=1000
python -m scripts.chat_eval -- -i mid
```

**SFT 1ì°¨: 1,000 ìŠ¤í…**

```bash
python -m scripts.chat_sft \
  --run=dummy \
  --device_batch_size=1 \
  --max_steps=1000
python -m scripts.chat_eval -- -i sft
```

---

### Day 4 (ì„ íƒ, 3h ë‚´ì™¸)

**(ì˜µì…˜) RL 500~1000 ìŠ¤í… + ë¦¬í¬íŠ¸/ì›¹UI**

```bash
# RL (ì˜µì…˜: GSM8K)
# python -m scripts.chat_rl -- --run=dummy  # ë ˆí¬ ë²„ì „ì— ë”°ë¼ ì¸ì ìƒì´ ê°€ëŠ¥

python -m nanochat.report generate   # ì¢…í•© ë¦¬í¬íŠ¸ (ë ˆí¬ ì§€ì› ì‹œ)
python -m scripts.chat_web
# http://127.0.0.1:8000
```

---

# ğŸŸ£ ì „ì²´(Full-ish) ë²„ì „ â€” ì•½ 3~4ì¼ / 6h ë¸”ë¡ Ã— 4~5íšŒ

### Day 1 (6h)

**Base Pretrain 1ì°¨: +2,000 ìŠ¤í… (ëˆ„ì  ~3,000)**

```bash
export PYTORCH_ALLOC_CONF="expandable_segments:True,max_split_size_mb:64,garbage_collection_threshold:0.8"

python -m scripts.base_train \
  --run=dummy \
  --depth=6 \
  --max_seq_len=384 \
  --device_batch_size=1 \
  --total_batch_size=393216 \
  --num_iterations=2000
```

**ë¶„ê¸° ì €ì¥**

```bash
CKPT=$(ls -1t runs/dummy/*.pt 2>/dev/null | head -n 1)
echo "LATEST CKPT: $CKPT"
mkdir -p ckpt_archive && [ -n "$CKPT" ] && cp -v "$CKPT" ckpt_archive/
```

---

### Day 2 (6h)

**Base Pretrain 2ì°¨: +2,000 ìŠ¤í… (ëˆ„ì  ~5,000 / ~1.96B í† í°)**

```bash
python -m scripts.base_train \
  --run=dummy \
  --max_seq_len=384 \
  --device_batch_size=1 \
  --total_batch_size=393216 \
  --num_iterations=2000
```

**ë¹ ë¥¸ í‰ê°€ & ë¶„ê¸° ì €ì¥**

```bash
python -m scripts.base_eval
python -m scripts.base_loss
CKPT=$(ls -1t runs/dummy/*.pt 2>/dev/null | head -n 1)
echo "LATEST CKPT: $CKPT"
mkdir -p ckpt_archive && [ -n "$CKPT" ] && cp -v "$CKPT" ckpt_archive/
```

---

### Day 3 (6h)

**Mid-Train 1ì°¨: 1,500 ìŠ¤í…** _(Full ê¶Œì¥: ì¡°ê¸ˆ ë” íƒœì›€)_

```bash
python -m scripts.mid_train \
  --run=dummy \
  --device_batch_size=1 \
  --max_steps=1500
python -m scripts.chat_eval -- -i mid
```

---

### Day 4 (6h)

**SFT 1ì°¨: 1,500 ìŠ¤í…**

```bash
python -m scripts.chat_sft \
  --run=dummy \
  --device_batch_size=1 \
  --max_steps=1500
python -m scripts.chat_eval -- -i sft
```

---

### Day 5 (ì„ íƒ, 3h ë‚´ì™¸)

**(ì˜µì…˜) RL 1000 ìŠ¤í… + ë¦¬í¬íŠ¸/ì›¹UI**

```bash
# RL (ì˜µì…˜)
# python -m scripts.chat_rl -- --run=dummy

python -m nanochat.report generate
python -m scripts.chat_web
# http://127.0.0.1:8000
```

---

## ğŸ§© ì‹¤ì „ íŒ (ê³µí†µ)

- **ë¦¬ì†ŒìŠ¤ ì—¬ìœ ê°€ í•„ìš”í•  ë•Œ**
  `--max_seq_len=256` + `--total_batch_size=262144` ë¡œ í•œ ë‹¨ê³„ ë‚®ì¶° ìŠ¤íŒŒì´í¬/í”„ë ˆì„ë“œë ì™„í™”.
- **WSL I/O ìµœì í™”**
  í”„ë¡œì íŠ¸/ë°ì´í„°ë¥¼ **ë¦¬ëˆ…ìŠ¤ í™ˆ(`~`)**ì— ë‘ê¸°(`/mnt/c` ê²½ìœ  I/O ì§€ì–‘).
- **ì›¹ì„œí•‘/ë™ì˜ìƒ ë³‘í–‰ ì‹œ**
  ë¸Œë¼ìš°ì € **í•˜ë“œì›¨ì–´ ê°€ì† OFF**, íƒ­ ìˆ˜ ìµœì†Œí™”, ì „ë ¥ ìº¡(`sudo nvidia-smi -pl 120`) ìœ ì§€.
- **ë¶„ê¸° ì‹¤í—˜**
  Dayë§ˆë‹¤ `ckpt_archive/`ì— ì²´í¬í¬ì¸íŠ¸ ë°±ì—… í›„, í•„ìš” ì‹œ ìƒˆ `--run`ìœ¼ë¡œ ê°€ì§€ì¹˜ê¸°.
  _`--init_from` ì˜µì…˜ì´ ì—†ìœ¼ë©´ ìƒëµí•˜ê³  ë™ì¼ `--run`ìœ¼ë¡œ ì´ì–´ì„œ í•™ìŠµ._
